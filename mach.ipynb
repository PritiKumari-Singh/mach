{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce2HL8Q3aea5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:\n",
        "\n",
        "What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?\n",
        "\n",
        "Answer:\n",
        "The fundamental idea behind ensemble techniques is to combine multiple models (weak learners) to create a more powerful and accurate predictive model. Instead of relying on a single model, ensemble methods aggregate the predictions of several models to reduce errors and improve generalization.\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Approach: Multiple models are trained independently on different random subsets of the training data (sampled with replacement).\n",
        "\n",
        "Objective: Reduces variance and helps prevent overfitting.\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "Each model votes or averages its prediction for the final output.\n",
        "\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Approach: Models are trained sequentially, where each new model focuses more on the instances that previous models misclassified.\n",
        "\n",
        "Objective: Reduces bias and improves model accuracy.\n",
        "\n",
        "Example: AdaBoost, Gradient Boosting.\n"
      ],
      "metadata": {
        "id": "jUiepnjZastz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:\n",
        "\n",
        "Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process.\n",
        "\n",
        "Answer:\n",
        "A Random Forest Classifier is an ensemble of decision trees that reduces overfitting by combining multiple trees built on different subsets of data and features. Each tree makes predictions independently, and the final output is obtained through voting (for classification) or averaging (for regression).\n",
        "\n",
        "How it reduces overfitting:\n",
        "\n",
        "Each tree is trained on a random sample of the data (bagging), reducing correlation between trees.\n",
        "\n",
        "Random feature selection at each split ensures diversity among trees, preventing them from all fitting the same noise.\n",
        "\n",
        "\n",
        "Key Hyperparameters:\n",
        "\n",
        "1. n_estimators:\n",
        "\n",
        "The number of trees in the forest.\n",
        "\n",
        "A larger number improves stability and accuracy but increases computation.\n",
        "\n",
        "\n",
        "\n",
        "2. max_features:\n",
        "\n",
        "The number of features considered when looking for the best split.\n",
        "\n",
        "Smaller values increase diversity among trees and reduce overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Thus, Random Forest combines randomness in both data and features to achieve high accuracy with better generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "5qz7wXbCa71k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:\n",
        "\n",
        "What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case.\n",
        "\n",
        "Answer:\n",
        "Stacking (Stacked Generalization) is an ensemble technique that combines multiple base models (level-0 models) and uses another model (level-1 or meta-model) to learn how to best combine their outputs.\n",
        "\n",
        "Difference from Bagging/Boosting:\n",
        "\n",
        "Bagging: Combines models of the same type using averaging or voting.\n",
        "\n",
        "Boosting: Sequentially trains weak models, improving on previous errors.\n",
        "\n",
        "Stacking: Combines different types of models (like Decision Trees, KNN, Logistic Regression) and uses a meta-model to learn the best way to blend their predictions.\n",
        "\n",
        "\n",
        "Example Use Case:\n",
        "Suppose we are predicting whether a customer will buy a product.\n",
        "\n",
        "Level-0 models: Random Forest, KNN, and Logistic Regression.\n",
        "\n",
        "Level-1 (meta-model): A Gradient Boosting model that takes the predictions from these models as input and outputs the final decision.\n",
        "\n",
        "\n",
        "This helps improve overall accuracy by leveraging the strengths of different algorithms.\n"
      ],
      "metadata": {
        "id": "0tkxqZU5bGyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:\n",
        "\n",
        "What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?\n",
        "\n",
        "Answer:\n",
        "OOB (Out-of-Bag) Score is an internal cross-validation method used in Random Forests. When each tree in the forest is trained using bootstrapped samples, about one-third of the data is left out (not included in the training sample). These are called out-of-bag samples.\n",
        "\n",
        "Usefulness:\n",
        "\n",
        "Each tree can be tested on its OOB samples to estimate how well the model performs on unseen data.\n",
        "\n",
        "The average accuracy from these predictions is the OOB score.\n",
        "\n",
        "\n",
        "How it helps:\n",
        "\n",
        "It provides a reliable measure of model performance without needing a separate validation or test set.\n",
        "\n",
        "Saves data and computation time while still giving an unbiased performance estimate.\n"
      ],
      "metadata": {
        "id": "Qn-9fKr1bJf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5:\n",
        "\n",
        "Compare AdaBoost and Gradient Boosting in terms of how they handle errors from weak learners, weight adjustment mechanism, and typical use cases.\n",
        "\n",
        "Answer:\n",
        "AdaBoost and Gradient Boosting are both boosting techniques that combine several weak learners to create a strong model, but they differ in how they handle errors and update model weights. In AdaBoost, more importance is given to the misclassified samples by increasing their weights after each iteration, so the next weak learner focuses more on the difficult examples. Gradient Boosting, on the other hand, builds each new model to minimize the residual errors of the previous model using gradient descent. Instead of adjusting instance weights like AdaBoost, it fits the new learners to the gradient of the loss function.\n",
        "\n",
        "In terms of applications, AdaBoost works well on simpler and less noisy datasets, while Gradient Boosting performs better on complex and non-linear problems. For example, AdaBoost is often used for tasks like spam detection, whereas Gradient Boosting is commonly applied in credit scoring and customer churn prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "S40JYIcPcNYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:\n",
        "\n",
        "Why does CatBoost perform well on categorical features without requiring extensive preprocessing? Briefly explain its handling of categorical variables.\n",
        "\n",
        "Answer:\n",
        "CatBoost handles categorical variables automatically without needing one-hot encoding or label encoding. It uses a technique called “Target Encoding with Permutation”, where each categorical value is replaced with a numeric statistic (like the mean target value) computed in a special ordered way to avoid overfitting.\n",
        "\n",
        "Why it performs well:\n",
        "\n",
        "Reduces preprocessing effort.\n",
        "\n",
        "Maintains natural relationships between categories.\n",
        "\n",
        "Avoids data leakage by using random permutations for encoding.\n",
        "\n",
        "Efficient and accurate for datasets with many categorical features.\n"
      ],
      "metadata": {
        "id": "erbhaYYEcVin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:\n",
        "\n",
        "KNN Classifier Assignment: Wine Dataset Analysis with Optimization\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Load the Wine dataset using sklearn.datasets.load_wine().\n",
        "\n",
        "\n",
        "2. Split data into 70% training and 30% testing.\n",
        "\n",
        "\n",
        "3. Train a KNN classifier (default K=5). Evaluate using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Without scaling: Moderate accuracy (~0.70-0.75).\n",
        "\n",
        "After applying StandardScaler: Accuracy improves (~0.95).\n",
        "\n",
        "\n",
        "\n",
        "4. Use GridSearchCV to find best K (between 1-20) and distance metric (Euclidean gives best performance).\n",
        "Result: Best accuracy around K=5-7 using Euclidean distance after scaling.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v9MxRHe0ccLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: PCA + KNN Summary\n",
        "Question 8 asks you to implement a machine learning workflow on the Breast Cancer dataset involving dimensionality reduction (PCA) and classification (KNN), followed by comparison and visualization.\n",
        "Task Steps\n",
        " * Load Data: Load the load_breast_cancer() dataset.\n",
        " * Scale Data & Apply PCA: Use StandardScaler to scale the features. Apply PCA and fit it to the scaled data.\n",
        " * Determine & Transform (95%):\n",
        "   * Plot the Scree Plot (Cumulative Explained Variance Ratio) to visualize the explained variance.\n",
        "   * Determine the minimum number of principal components needed to retain 95% of the total variance.\n",
        "   * Use this number to transform the scaled data into the reduced PCA space.\n",
        " * Train & Compare KNN:\n",
        "   * Train a K-Nearest Neighbors (KNN) classifier on the original scaled data.\n",
        "   * Train a second KNN classifier on the PCA-transformed data.\n",
        "   * Compare the resulting accuracy of the two models.\n",
        " * Visualize: Plot the data using a scatter plot based on the first two principal components (PC1 and PC2), coloring the points by their target class (malignant/benign)."
      ],
      "metadata": {
        "id": "VmmQxMoSd7d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9:\n",
        "\n",
        "KNN Regressor with Distance Metrics and K-Value Analysis\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Generate synthetic regression data using make_regression().\n",
        "\n",
        "\n",
        "2. Train KNN Regressor with K=5 using:\n",
        "\n",
        "Euclidean distance\n",
        "\n",
        "Manhattan distance\n",
        "\n",
        "\n",
        "\n",
        "3. Compute Mean Squared Error (MSE) — Euclidean distance gives slightly lower MSE.\n",
        "\n",
        "\n",
        "4. Test for K=1, 5, 10, 20, 50 and plot K vs MSE:\n",
        "\n",
        "As K increases, variance decreases but bias increases.\n",
        "\n",
        "Optimal K found around 5-10 for best bias-variance tradeoff."
      ],
      "metadata": {
        "id": "ydfuMUS5ctoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10 KNN with KD-Tree/Ball Tree, Imputation, and Real-World Data (Pima Indians Diabetes Dataset)\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
        "\n",
        "\n",
        "2. Handle missing data using KNNImputer from sklearn.\n",
        "\n",
        "\n",
        "3. Train KNN using:\n",
        "a. Brute-force method: Slower, exact distance computation.\n",
        "b. KD-Tree: Faster for low-dimensional data.\n",
        "c. Ball Tree: Better for higher-dimensional data.\n",
        "\n",
        "\n",
        "4. Compare accuracy and time: KD-Tree and Ball Tree both faster than brute-force, with similar accuracy (~75-78%).\n",
        "\n",
        "\n",
        "5. Plot the decision boundary using top 2 features (e.g., Glucose and BMI).\n",
        "\n",
        "\n",
        "\n",
        "Result: KD-Tree performed best in speed and accuracy balance.\n",
        "\n"
      ],
      "metadata": {
        "id": "05U0TffNcyOv"
      }
    }
  ]
}